{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import important libraries\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sklearn\n",
    "import string\n",
    "import re, os, io\n",
    "from numpy import array, argmax, random, take\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.14.0'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_en_1 = \"data1/english.txt\"\n",
    "filepath_te_1 = \"data1/telugu.txt\"\n",
    "filepath_en_2 = \"data2/english.txt\"\n",
    "filepath_te_2 = \"data2/telugu.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples in Telugu Language Data1: 5000\n",
      "Number of Examples in English Language Data1: 5000\n",
      "Number of Examples in Telugu Language Data2: 4499\n",
      "Number of Examples in English Language Data2: 4499\n"
     ]
    }
   ],
   "source": [
    "tel_text_1  = io.open(filepath_te_1,encoding='UTF-8').read().strip().split('\\n')\n",
    "eng_text_1  = io.open(filepath_en_1).read().strip().split('\\n')\n",
    "print(\"Number of Examples in Telugu Language Data1:\",len(tel_text_1))\n",
    "print(\"Number of Examples in English Language Data1:\",len(eng_text_1))\n",
    "\n",
    "tel_text_2  = io.open(filepath_te_2,encoding='UTF-16').read().strip().split('\\n')\n",
    "eng_text_2  = io.open(filepath_en_2).read().strip().split('\\n')\n",
    "print(\"Number of Examples in Telugu Language Data2:\",len(tel_text_2))\n",
    "print(\"Number of Examples in English Language Data2:\",len(eng_text_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"ఈ సందర్భంగా టీమిండియా మాజీ కెప్టెన్\\u200c 'ది వాల్' రాహుల్\\u200c ద్రావిడ్\\u200c తన ఫేవరెట్\\u200c క్రికెటర్\\u200c అని దీపికా తెలిపారు.\", 'డ్రైనెస్ ను తొలగించాలి.', 'శ్రీదేవి ఆకస్మిక మరణం దేశవ్యాప్తంగా ప్రతీ ఒక్కరిని శోక సంద్రంలో ముంచెత్తింది.', 'అలాంటి యత్నమే ఇది.', '\"5 మెగా పిక్సల్ ఫ్రట్ ఫేసింగ్ కెమెరా,\"']\n",
      "\n",
      "['\"speaking about her all-time favourite cricketer, deepika said its former india skipper rahul dravid.\"', 'debris has to be removed.', \"sridevi's sudden demise sent shock waves throughout the country.\", 'this is one such attempt.', '5-megapixel front camera']\n",
      "['\"రామాయణం భారతీయ వాఙ్మయంలో ఆదికావ్యంగాను, దానిని సంస్కృతంలో రచించిన వాల్మీకి మహాముని ఆదికవిగాను సుప్రసిద్ధం.\"', '\"సాహిత్య చరిత్ర ప్రకారం రామాయణ కావ్యం వేద కాలం తర్వాత, అనగా సుమారు సా.శ.\"', 'రామాయణం కావ్యంలోని కథ త్రేతాయుగం కాలంలో జరిగినట్లు వాల్మీకి పేర్కొన్నాడు.', '\"భారతదేశం లోని అన్ని భాషల యందు, అన్ని ప్రాంతాలనందు ఈ కావ్యం ఎంతో ఆదరణీయం, పూజనీయం.\"', '\"ఇండోనేషియా, థాయిలాండ్, కంబోడియా, మలేషియా, వియత్నాం, లావోస్ దేశాలలో కూడా రామాయణ గాథ ప్రచారంలో ఉంది.\"']\n",
      "\n",
      "['\"The Ramayana is the first poem in Indian literature, and Valmiki Mahamuni, who wrote it in Sanskrit, is well known as the first poem.\"', '\"According to literary history, the Ramayana poem was written after the Vedic period, i.e. around a.d.\"', 'Valmiki states that the story of the Ramayana takes place during the Treta Yuga.', 'This poem is highly respected and revered in all languages ??and regions of India.', '\"The story of Ramayana is also being propagated in Indonesia, Thailand, Cambodia, Malaysia, Vietnam and Laos.\"']\n"
     ]
    }
   ],
   "source": [
    "sample_tel_1 = tel_text_1[0:5]\n",
    "sample_eng_1 = eng_text_1[0:5]\n",
    "print(sample_tel_1)\n",
    "print()\n",
    "print(sample_eng_1)\n",
    "\n",
    "sample_tel_2 = tel_text_2[0:5]\n",
    "sample_eng_2 = eng_text_2[0:5]\n",
    "print(sample_tel_2)\n",
    "print()\n",
    "print(sample_eng_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<start> speaking about her all time favourite cricketer , deepika said its former india skipper rahul dravid . <end>',\n",
       " '<start> debris has to be removed . <end>',\n",
       " '<start> sridevi s sudden demise sent shock waves throughout the country . <end>',\n",
       " '<start> this is one such attempt . <end>',\n",
       " '<start> megapixel front camera <end>')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c)!='BN')\n",
    "\n",
    "# Remove punctuations, reduce words to lower case,and adding '<start>' and '<end>' tokens to the sentences\n",
    "\n",
    "def preprocess_eng(w):\n",
    "    #w= unicode_to_ascii(w)\n",
    "    w= w.lower().strip()\n",
    "    w= re.sub(r'([?.!,¿_])',r' \\1 ',w)\n",
    "    w= re.sub(r'[\" \"]+', \" \",w) # keep white space\n",
    "    w= re.sub(r\"[^a-zA-Z?.!,¿_]+\",\" \",w)\n",
    "    w= w.strip()\n",
    "    w= '<start> '+ w +' <end>'\n",
    "    return w\n",
    "\n",
    "def preprocess_tel(w):\n",
    "    w= unicode_to_ascii(w)\n",
    "    w= re.sub(r'([\\?.!,¿_])',r' \\1 ',w)\n",
    "    w= re.sub(r'[\" \"]+', \" \",w) # keep white space\n",
    "    w= w.strip()\n",
    "    w= '<start> '+ w +' <end>'\n",
    "    w= w.replace(\"u'\", \"'\")\n",
    "    return w\n",
    "\n",
    "line=[preprocess_eng(i) for i in sample_eng_1]\n",
    "tuple(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eng_dataset(filepath):\n",
    "    #limit num_examples for faster training - len(lines) is full data\n",
    "    lines= io.open(filepath,encoding='UTF-8').read().strip().split('\\n')\n",
    "    lines=[preprocess_eng(i) for i in lines]\n",
    "    return tuple(lines)\n",
    "\n",
    "\n",
    "def create_tel_dataset(filepath):\n",
    "    #limit num_examples for faster training - len(lines) is full data\n",
    "    if filepath==filepath_te_1:\n",
    "        lines= io.open(filepath,encoding='UTF-8').read().strip().split('\\n')\n",
    "    else:\n",
    "        lines= io.open(filepath,encoding='UTF-16').read().strip().split('\\n')\n",
    "    lines=[preprocess_tel(i) for i in lines]\n",
    "    return tuple(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize function \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = Tokenizer(filters='',oov_token='<OOV>')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    lang_data = lang_tokenizer.texts_to_sequences(lang)\n",
    "    lang_data = pad_sequences(lang_data,padding=\"post\")\n",
    "    return lang_data, lang_tokenizer\n",
    "\n",
    "def load_dataset(filepath1,filepath2):\n",
    "    # create cleaned input and target sentences\n",
    "    input_lang = create_eng_dataset(filepath1)\n",
    "    targ_lang  = create_tel_dataset(filepath2)\n",
    "    input_lang, inp_lang_tokenizer  = tokenize(input_lang)\n",
    "    target_lang, targ_lang_tokenizer= tokenize(targ_lang)\n",
    "    return input_lang,target_lang,inp_lang_tokenizer,targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_lang_1,target_lang_1,inp_lang_tokenizer_1,targ_lang_tokenizer_1 = load_dataset(filepath_en_1,filepath_te_1)\n",
    "\n",
    "input_lang_2,target_lang_2,inp_lang_tokenizer_2,targ_lang_tokenizer_2 = load_dataset(filepath_en_2,filepath_te_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4499, 92) (4499, 75)\n",
      "(92,)\n"
     ]
    }
   ],
   "source": [
    "input_lang_1.shape,target_lang_1.shape\n",
    "print(input_lang_2.shape,target_lang_2.shape)\n",
    "input_lang_1[0].shape\n",
    "print(input_lang_2[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_train_1, input_val_1, target_train_1, target_val_1 = train_test_split(input_lang_1, target_lang_1, test_size=0.2)\n",
    "input_train_2, input_val_2, target_train_2, target_val_2 = train_test_split(input_lang_2, target_lang_2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 92), (4000, 95))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train_1.shape,target_train_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3599, 92), (3599, 75))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_train_2.shape,target_train_2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(64, 92), dtype=tf.int32, name=None), TensorSpec(shape=(64, 75), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer_size = 5000\n",
    "batch_size  = 64\n",
    "\n",
    "train_dataset_1 = tf.data.Dataset.from_tensor_slices((input_train_1,target_train_1))\n",
    "train_dataset_1 = train_dataset_1.shuffle(buffer_size).batch(batch_size,drop_remainder =True)\n",
    "\n",
    "train_dataset_2 = tf.data.Dataset.from_tensor_slices((input_train_2,target_train_2))\n",
    "train_dataset_2 = train_dataset_2.shuffle(buffer_size).batch(batch_size,drop_remainder =True)\n",
    "\n",
    "train_dataset_1\n",
    "train_dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_1 = tf.data.Dataset.from_tensor_slices((input_val_1,target_val_1))\n",
    "val_dataset_1 = val_dataset_1.batch(batch_size,drop_remainder =True)\n",
    "\n",
    "val_dataset_2 = tf.data.Dataset.from_tensor_slices((input_val_2,target_val_2))\n",
    "val_dataset_2 = val_dataset_2.batch(batch_size,drop_remainder =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset2: max_length_english, max_length_telugu, vocab_size_english, vocab_size_telugu\n",
      "92 75 5949 11797\n"
     ]
    }
   ],
   "source": [
    "ex_input_batch_1,ex_tar_batch_1 = next(iter(train_dataset_1))\n",
    "ex_input_batch_1.shape,ex_tar_batch_1.shape\n",
    "\n",
    "maxlen_input_1 = ex_input_batch_1.shape[1]\n",
    "maxlen_output_1   = ex_tar_batch_1.shape[1]\n",
    "\n",
    "vocab_inp_size_1 = len(inp_lang_tokenizer_1.word_index)+1\n",
    "vocab_tar_size_1 = len(targ_lang_tokenizer_1.word_index)+1\n",
    "\n",
    "ex_input_batch_2,ex_tar_batch_2 = next(iter(train_dataset_2))\n",
    "ex_input_batch_2.shape,ex_tar_batch_2.shape\n",
    "\n",
    "maxlen_input_2 = ex_input_batch_2.shape[1]\n",
    "maxlen_output_2   = ex_tar_batch_2.shape[1]\n",
    "\n",
    "vocab_inp_size_2 = len(inp_lang_tokenizer_2.word_index)+1\n",
    "vocab_tar_size_2 = len(targ_lang_tokenizer_2.word_index)+1\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "steps_per_epoch_1 = 5000//batch_size\n",
    "steps_per_epoch_2= 4270//batch_size\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset2: max_length_english, max_length_telugu, vocab_size_english, vocab_size_telugu\")\n",
    "print(maxlen_input_2, maxlen_output_2, vocab_inp_size_2, vocab_tar_size_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, h, c = self.lstm_layer(x, initial_state=hidden)\n",
    "        return output, h, c\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return [tf.zeros((self.batch_size, self.enc_units)), tf.zeros((self.batch_size, self.enc_units))]  # all zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2\n",
      "Encoder output shape: (batch size, sequence length, units) (64, 92, 512)\n",
      "Encoder h vecotr shape: (batch size, units) (64, 512)\n",
      "Encoder c vector shape: (batch size, units) (64, 512)\n"
     ]
    }
   ],
   "source": [
    "## Test Encoder Stack\n",
    "\n",
    "\n",
    "encoder_2 = Encoder(vocab_inp_size_2, embedding_dim, units,batch_size)\n",
    "\n",
    "\n",
    "# sample input\n",
    "sample_hidden_2 = encoder_2.initialize_hidden_state()\n",
    "sample_output_2, sample_h_2, sample_c_2 = encoder_2(ex_input_batch_2, sample_hidden_2)\n",
    "print ('Dataset 2')\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output_2.shape))\n",
    "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h_2.shape))\n",
    "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c_2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "628"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "units=512\n",
    "batch_size =64\n",
    "maxlen_output_1 = target_train_1.shape[1]\n",
    "maxlen_output_2 = target_train_2.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.units = units\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.decoder_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #Final Dense layer on which softmax will be applied\n",
    "        self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # Define the fundamental cell for decoder recurrent structure\n",
    "        self.decoder_inner_cell = tf.keras.layers.LSTMCell(self.units)\n",
    "        \n",
    "        # Sampler\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "        \n",
    "        ## Attention Wrapper ##\n",
    "        self.attention_mechanism = tfa.seq2seq.attention_wrapper.LuongAttention(units,memory=None,\n",
    "                                                                                memory_sequence_length=self.batch_size*[maxlen_input_2])\n",
    "        self.attention_decoder_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper(cell=self.decoder_inner_cell, \n",
    "                                                                                     attention_mechanism=self.attention_mechanism,\n",
    "                                                                                     attention_layer_size=self.units)\n",
    "        \n",
    "        ## Basic Decoder ##\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(cell=self.attention_decoder_cell,sampler = self.sampler,\n",
    "                                               output_layer = self.output_layer)\n",
    "        \n",
    "         \n",
    "        self.inference_decoder = tfa.seq2seq.BasicDecoder(cell=self.attention_decoder_cell,\n",
    "                                                          sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
    "                                                              embedding_fn = self.decoder_embedding),\n",
    "                                                          output_layer=self.output_layer,maximum_iterations = 154)\n",
    "        \n",
    "    def build_initial_state(self, batch_size, encoder_state, Dtype):\n",
    "        decoder_initial_state = self.attention_decoder_cell.get_initial_state(batch_size=batch_size, dtype=Dtype)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "        return decoder_initial_state\n",
    "        \n",
    "    def call(self,decoder_input,initial_state):\n",
    "        decoder_embeddings = self.decoder_embedding(decoder_input)\n",
    "        outputs, _, _ = self.decoder(decoder_embeddings,initial_state=initial_state,\n",
    "                                     sequence_length=self.batch_size*[maxlen_output_2-1])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder1 Outputs Shape:  (64, 74, 11797)\n"
     ]
    }
   ],
   "source": [
    "# Test decoder stack\n",
    "\n",
    "decoder_2 = Decoder(vocab_tar_size_2, embedding_dim,512, batch_size)\n",
    "sample_x_2 = tf.random.uniform((batch_size, maxlen_output_2))\n",
    "decoder_2.attention_mechanism.setup_memory(sample_output_2) # from encoder\n",
    "initial_state_1 = decoder_2.build_initial_state(batch_size, [sample_h_2, sample_c_2], tf.float32)\n",
    "\n",
    "sample_decoder_outputs_2 = decoder_2(sample_x_2, initial_state_1)\n",
    "\n",
    "print(\"Decoder1 Outputs Shape: \", sample_decoder_outputs_2.rnn_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # real shape = (batch_size, maxlen_output)\n",
    "    # pred shape = (batch_size, maxlen_output, vocab_tar_size )\n",
    "    cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "    mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)  \n",
    "    loss = mask* loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_2 = './training_checkpoints_1'\n",
    "checkpoint_prefix_2 = os.path.join(checkpoint_dir_2, \"ckpt\")\n",
    "checkpoint_2 = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder_2,\n",
    "                                 decoder=decoder_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(encoder, decoder,inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "        \n",
    "        dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
    "        real = targ[ : , 1: ]         # ignore <start> token\n",
    "        \n",
    "        # Set the AttentionMechanism object with encoder_outputs\n",
    "        decoder.attention_mechanism.setup_memory(enc_output)\n",
    "        # Create AttentionWrapperState as initial_state for decoder\n",
    "        decoder_initial_state = decoder.build_initial_state(batch_size, [enc_h, enc_c], tf.float32)\n",
    "        pred = decoder(dec_input, decoder_initial_state)\n",
    "        logits = pred.rnn_output\n",
    "        loss = loss_function(real, logits)\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) >= self.buffer_size:\n",
    "            self.buffer.pop(0)  # Remove the oldest experience\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "\n",
    "import time\n",
    "\n",
    "def train_model_with_replay(input_train1, target_train1, input_train2, target_train2, encoder, decoder, epochs, steps_per_epoch, batch_size, replay_buffer):\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        enc_hidden = encoder.initialize_hidden_state()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Combine datasets\n",
    "        combined_input = np.concatenate([input_train1, input_train2], axis=0)\n",
    "        combined_target = np.concatenate([target_train1, target_train2], axis=0)\n",
    "\n",
    "        # Define the train_dataset\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((combined_input, combined_target)).shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "        # Train with mixed samples\n",
    "        for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "            batch_loss = train_step(\n",
    "                encoder, decoder, inp, targ, enc_hidden\n",
    "            )\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            # Add to replay buffer after training on first dataset\n",
    "            if batch < len(input_train1) // batch_size:\n",
    "                replay_buffer.add((inp, targ))\n",
    "\n",
    "            # Mix with replay buffer samples during training on second dataset\n",
    "            if batch >= len(input_train1) // batch_size:\n",
    "                if replay_buffer.size() > 0:\n",
    "                    replay_batch = replay_buffer.sample(batch_size)\n",
    "                    replay_input, replay_target = zip(*replay_batch)\n",
    "                    mixed_input = np.concatenate([inp, np.array(replay_input)], axis=0)\n",
    "                    mixed_target = np.concatenate([targ, np.array(replay_target)], axis=0)\n",
    "\n",
    "                    # Train with mixed data (from second dataset + replayed first dataset)\n",
    "                    batch_loss = train_step(\n",
    "                        encoder, decoder, mixed_input, mixed_target, enc_hidden\n",
    "                    )\n",
    "                    total_loss += batch_loss\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                print(f'Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy()}')\n",
    "\n",
    "        print(f'Epoch {epoch + 1} Loss {total_loss / steps_per_epoch}')\n",
    "        print(f'Time taken for 1 epoch {time.time() - start} sec\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.2946\n",
      "Epoch 1 Loss 0.9923\n",
      "Time taken for 1 epoch 444.6845715045929 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.0116\n",
      "Epoch 2 Loss 0.8857\n",
      "Time taken for 1 epoch 450.36882615089417 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.1224\n",
      "Epoch 3 Loss 0.8326\n",
      "Time taken for 1 epoch 449.26217555999756 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.9923\n",
      "Epoch 4 Loss 0.7937\n",
      "Time taken for 1 epoch 437.66591715812683 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.9658\n",
      "Epoch 5 Loss 0.7527\n",
      "Time taken for 1 epoch 431.86577677726746 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.7402\n",
      "Epoch 6 Loss 0.7119\n",
      "Time taken for 1 epoch 433.3895616531372 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.7080\n",
      "Epoch 7 Loss 0.6676\n",
      "Time taken for 1 epoch 433.9366111755371 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.7305\n",
      "Epoch 8 Loss 0.6199\n",
      "Time taken for 1 epoch 433.54721212387085 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.6322\n",
      "Epoch 9 Loss 0.5637\n",
      "Time taken for 1 epoch 432.2295410633087 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.6064\n",
      "Epoch 10 Loss 0.5087\n",
      "Time taken for 1 epoch 433.5595180988312 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.5857\n",
      "Epoch 11 Loss 0.4498\n",
      "Time taken for 1 epoch 433.6910619735718 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.4457\n",
      "Epoch 12 Loss 0.3955\n",
      "Time taken for 1 epoch 433.5330216884613 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.4127\n",
      "Epoch 13 Loss 0.3471\n",
      "Time taken for 1 epoch 431.6806437969208 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.3344\n",
      "Epoch 14 Loss 0.3002\n",
      "Time taken for 1 epoch 433.1579668521881 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.3008\n",
      "Epoch 15 Loss 0.2646\n",
      "Time taken for 1 epoch 432.73012018203735 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = ReplayBuffer(buffer_size=500)\n",
    "\n",
    "EPOCHS = 15\n",
    "train_model_with_replay(\n",
    "    input_train_1,\n",
    "    target_train_1,\n",
    "    input_train_2,\n",
    "    target_train_2,\n",
    "    encoder_2,\n",
    "    decoder_2,\n",
    "    EPOCHS,\n",
    "    steps_per_epoch_2,  # Example value\n",
    "    batch_size,  # Example value\n",
    "    replay_buffer=replay_buffer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_evaluate_sentence(sentence,encoder,decoder,inp_lang_tokenizer,maxlen_input,targ_lang_tokenizer,beam_width=3):\n",
    "    y = str(sentence)\n",
    "    y = preprocess_eng(y)\n",
    "    sentence= str(y)\n",
    "\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.strip().split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=maxlen_input,padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "    \n",
    "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "    \n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "    \n",
    "    start_tokens = tf.fill([inference_batch_size], targ_lang_tokenizer.word_index['<start>'])\n",
    "    end_token = targ_lang_tokenizer.word_index['<end>']\n",
    "    \n",
    "    enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n",
    "    decoder.attention_mechanism.setup_memory(enc_out)\n",
    "    print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 194, 512]] :\", enc_out.shape)\n",
    "    \n",
    "    # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
    "    hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n",
    "    decoder_initial_state = decoder.attention_decoder_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n",
    "    decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n",
    "    \n",
    "    # Instantiate BeamSearchDecoder\n",
    "    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.attention_decoder_cell,beam_width=beam_width, output_layer=decoder.output_layer)\n",
    "    decoder_embedding_matrix = decoder.decoder_embedding.variables[0]\n",
    "    \n",
    "    # The BeamSearchDecoder object's call() function takes care of everything.\n",
    "    outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, \n",
    "                                                              start_tokens=start_tokens, end_token=end_token, \n",
    "                                                              initial_state=decoder_initial_state)\n",
    "    \n",
    "    final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n",
    "    beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n",
    "    return final_outputs.numpy(), beam_scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_translate2(sentence, targ_lang_tokenizer):\n",
    "    result, beam_scores = beam_evaluate_sentence(sentence,encoder_2,decoder_2,inp_lang_tokenizer_2,maxlen_input_2,targ_lang_tokenizer)\n",
    "    print(result.shape, beam_scores.shape)\n",
    "    print()\n",
    "    \n",
    "    for beam, score in zip(result, beam_scores):\n",
    "        print(beam.shape, score.shape)\n",
    "        print()\n",
    "        \n",
    "    output = targ_lang_tokenizer.sequences_to_texts(beam)\n",
    "    output = [a[:a.index('<end>')] for a in output]\n",
    "    beam_score = [a.sum() for a in score]\n",
    "    print('Input: %s' % (sentence))\n",
    "    print()\n",
    "    \n",
    "    for i in range(len(output)):\n",
    "        print('{} Predicted translation: {}  {}'.format(i+1, output[i], beam_score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset2:\n",
      "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 194, 512]] : (3, 92, 512)\n",
      "(1, 3, 8) (1, 3, 8)\n",
      "\n",
      "(3, 8) (3, 8)\n",
      "\n",
      "Input: Good omens appeared to Sita.\n",
      "\n",
      "1 Predicted translation: ఇన్న నమ్ముకున్నాను !   -40.01940155029297\n",
      "2 Predicted translation: ఇన్న సహాయంగా ఇంద్రుడు మాతలిని సారథిగా పంపాడు .   -42.65491485595703\n",
      "3 Predicted translation: ఇన్న సమస్యలన్నీ చిటికెలో పరిష్కరమవాలంటే .   -46.907318115234375\n",
      "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 194, 512]] : (3, 92, 512)\n",
      "(1, 3, 12) (1, 3, 12)\n",
      "\n",
      "(3, 12) (3, 12)\n",
      "\n",
      "Input: Valmiki then asks Narada a question.\n",
      "\n",
      "1 Predicted translation: ఆ అత్యుత్సాహంలో , భానుమతి ఏమైనా మితిమీరి పెట్టారు .   -89.1968002319336\n",
      "2 Predicted translation: ఆ అత్యుత్సాహంలో , భానుమతి కూడా మితిమీరి , భ్రూమధ్యంబున అకస్మాత్తుగా .   -104.5013427734375\n",
      "3 Predicted translation: ఆ అత్యుత్సాహంలో , భానుమతి ఏమైనా మితిమీరి , భ్రూమధ్యంబున అకస్మాత్తుగా ఉంది .   -110.79620361328125\n"
     ]
    }
   ],
   "source": [
    "print('dataset2:')\n",
    "beam_translate2(u'Good omens appeared to Sita.',targ_lang_tokenizer_2)\n",
    "\n",
    "beam_translate2(u'Valmiki then asks Narada a question.',targ_lang_tokenizer_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data (if necessary)\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def evaluate_sen(sentence,encoder,decoder,inp_lang_tokenizer,maxlen_input,targ_lang_tokenizer):\n",
    "    y = str(sentence)\n",
    "    y = preprocess_eng(y)\n",
    "    sentence= str(y)\n",
    "\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=maxlen_input,padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "    \n",
    "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "    \n",
    "    start_tokens = tf.fill([inference_batch_size], targ_lang_tokenizer.word_index['<start>'])\n",
    "    end_token = targ_lang_tokenizer.word_index['<end>']\n",
    "    \n",
    "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "    \n",
    "    # Instantiate BasicDecoder object\n",
    "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.attention_decoder_cell, sampler=greedy_sampler, \n",
    "                                                output_layer=decoder.output_layer)\n",
    "    \n",
    "    # Setup Memory in decoder stack\n",
    "    decoder.attention_mechanism.setup_memory(enc_out)\n",
    "    \n",
    "    # set decoder_initial_state\n",
    "    decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
    "    decoder_embedding_matrix = decoder.decoder_embedding.variables[0]\n",
    "    \n",
    "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, \n",
    "                                     end_token= end_token, initial_state=decoder_initial_state)\n",
    "    return outputs.sample_id.numpy()\n",
    "\n",
    "\n",
    "def translate2(sentence,targ_lang_tokenizer,reference):\n",
    "  result = evaluate_sen(sentence,encoder_2,decoder_2,inp_lang_tokenizer_2,maxlen_input_2,targ_lang_tokenizer)\n",
    "  print(result)\n",
    "  result = targ_lang_tokenizer.sequences_to_texts(result)\n",
    "  \n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "  \n",
    "  # Calculate BLEU score\n",
    "  bleu = calculate_bleu_score(reference, result)\n",
    "  print(f\"BLEU Score: {bleu:.4f}\")\n",
    "  \n",
    "\n",
    "def calculate_bleu_score(reference, prediction):\n",
    "    \n",
    "    if isinstance(reference, list):\n",
    "        reference = \" \".join(reference)\n",
    "    if isinstance(prediction, list):\n",
    "        prediction = \" \".join(prediction)\n",
    "    prediction = prediction.replace('<end>', '').strip()\n",
    "    # Tokenize the reference and prediction sentences\n",
    "    reference_tokens = word_tokenize(reference)\n",
    "    prediction_tokens = word_tokenize(prediction)\n",
    "\n",
    "    # Wrap reference tokens in a list (BLEU requires list of lists for references)\n",
    "    references = [reference_tokens]\n",
    "\n",
    "    # Calculate the BLEU score using sentence_bleu\n",
    "    bleu_score = sentence_bleu(references, prediction_tokens)\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset2:\n",
      "[[1023 1694    4    3]]\n",
      "Input: They worship.\n",
      "Predicted translation: ['పూజలు చేస్తుంటారు . <end>']\n",
      "BLEU Score: 0.1000\n"
     ]
    }
   ],
   "source": [
    "print('dataset2:')\n",
    "# Example references (true translations, already in Telugu)\n",
    "reference1 = 'పూజలు చేస్తుంటారు.'\n",
    "translate2(u'They worship.',targ_lang_tokenizer_2,reference1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 14 152  98 465  10   3]]\n",
      "Input: And do you have it?\n",
      "Predicted translation: ['మీరు దానిని ఎందుకు కావాలి ? <end>']\n",
      "BLEU Score: 0.1300\n"
     ]
    }
   ],
   "source": [
    "reference2='మరి నీవద్ద అది ఉందా?'\n",
    "translate2(u'And do you have it?',targ_lang_tokenizer_2,reference2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1:\n",
      "[[5550 5551 5552 5553    4    3]]\n",
      "Input: he is a student of iit at bombay.\n",
      "Predicted translation: ['వ్యాపారస్తుడిని రాజుగారి ముందరు నిలపెట్టారు . <end>']\n",
      "BLEU Score: 0.2300\n",
      "[[  13 3349    5  662   19 1493 1576 3350    4    3]]\n",
      "Input: demand equal pay for equal work.\n",
      "Predicted translation: ['ఆ అత్యుత్సాహంలో , భానుమతి కూడా మితిమీరి పోలి సేవించింది . <end>']\n",
      "BLEU Score: 0.1830\n",
      "[[1668    4    3]]\n",
      "Input: the officials were shocked.\n",
      "Predicted translation: ['మంథాల్ . <end>']\n",
      "BLEU Score: 0.0900\n"
     ]
    }
   ],
   "source": [
    "print('dataset1:')\n",
    "# Example references (true translations, already in Telugu)\n",
    "reference1 = 'బాబు ముంబయిలో ఐఐటి చదువుతున్నాడు.'\n",
    "reference2='సమాన పనికి సమాన వేతనం ఇవ్వాలన్నారు.'\n",
    "reference3='దీంతో అధికారులు అలెర్ట్ అయ్యారు'\n",
    "translate2(u'he is a student of iit at bombay.',targ_lang_tokenizer_2,reference1)\n",
    "translate2(u'demand equal pay for equal work.',targ_lang_tokenizer_2,reference2)\n",
    "translate2(u'the officials were shocked.',targ_lang_tokenizer_2, reference3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
