{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import important libraries\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sklearn\n",
    "import string\n",
    "import re, os, io\n",
    "from numpy import array, argmax, random, take\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.14.0'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_en_1 = \"data1/english.txt\"\n",
    "filepath_te_1 = \"data1/telugu.txt\"\n",
    "filepath_en_2 = \"data2/english.txt\"\n",
    "filepath_te_2 = \"data2/telugu.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples in Telugu Language Data1: 5000\n",
      "Number of Examples in English Language Data1: 5000\n",
      "Number of Examples in Telugu Language Data2: 4270\n",
      "Number of Examples in English Language Data2: 4270\n"
     ]
    }
   ],
   "source": [
    "tel_text_1  = io.open(filepath_te_1,encoding='UTF-8').read().strip().split('\\n')\n",
    "eng_text_1  = io.open(filepath_en_1).read().strip().split('\\n')\n",
    "print(\"Number of Examples in Telugu Language Data1:\",len(tel_text_1))\n",
    "print(\"Number of Examples in English Language Data1:\",len(eng_text_1))\n",
    "\n",
    "tel_text_2  = io.open(filepath_te_2,encoding='UTF-16').read().strip().split('\\n')\n",
    "eng_text_2  = io.open(filepath_en_2).read().strip().split('\\n')\n",
    "print(\"Number of Examples in Telugu Language Data2:\",len(tel_text_2))\n",
    "print(\"Number of Examples in English Language Data2:\",len(eng_text_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"ఈ సందర్భంగా టీమిండియా మాజీ కెప్టెన్\\u200c 'ది వాల్' రాహుల్\\u200c ద్రావిడ్\\u200c తన ఫేవరెట్\\u200c క్రికెటర్\\u200c అని దీపికా తెలిపారు.\", 'డ్రైనెస్ ను తొలగించాలి.', 'శ్రీదేవి ఆకస్మిక మరణం దేశవ్యాప్తంగా ప్రతీ ఒక్కరిని శోక సంద్రంలో ముంచెత్తింది.', 'అలాంటి యత్నమే ఇది.', '\"5 మెగా పిక్సల్ ఫ్రట్ ఫేసింగ్ కెమెరా,\"']\n",
      "\n",
      "['\"speaking about her all-time favourite cricketer, deepika said its former india skipper rahul dravid.\"', 'debris has to be removed.', \"sridevi's sudden demise sent shock waves throughout the country.\", 'this is one such attempt.', '5-megapixel front camera']\n",
      "['\"రామాయణం భారతీయ వాఙ్మయంలో ఆదికావ్యంగాను, దానిని సంస్కృతంలో రచించిన వాల్మీకి మహాముని ఆదికవిగాను సుప్రసిద్ధం.\"', '\"సాహిత్య చరిత్ర ప్రకారం రామాయణ కావ్యం వేద కాలం తర్వాత, అనగా సుమారు సా.శ.\"', 'రామాయణం కావ్యంలోని కథ త్రేతాయుగం కాలంలో జరిగినట్లు వాల్మీకి పేర్కొన్నాడు.', '\"భారతదేశం లోని అన్ని భాషల యందు, అన్ని ప్రాంతాలనందు ఈ కావ్యం ఎంతో ఆదరణీయం, పూజనీయం.\"', '\"ఇండోనేషియా, థాయిలాండ్, కంబోడియా, మలేషియా, వియత్నాం, లావోస్ దేశాలలో కూడా రామాయణ గాథ ప్రచారంలో ఉంది.\"']\n",
      "\n",
      "['\"The Ramayana is the first poem in Indian literature, and Valmiki Mahamuni, who wrote it in Sanskrit, is well known as the first poem.\"', '\"According to literary history, the Ramayana poem was written after the Vedic period, i.e. around a.d.\"', 'Valmiki states that the story of the Ramayana takes place during the Treta Yuga.', 'This poem is highly respected and revered in all languages ??and regions of India.', '\"The story of Ramayana is also being propagated in Indonesia, Thailand, Cambodia, Malaysia, Vietnam and Laos.\"']\n"
     ]
    }
   ],
   "source": [
    "sample_tel_1 = tel_text_1[0:5]\n",
    "sample_eng_1 = eng_text_1[0:5]\n",
    "print(sample_tel_1)\n",
    "print()\n",
    "print(sample_eng_1)\n",
    "\n",
    "sample_tel_2 = tel_text_2[0:5]\n",
    "sample_eng_2 = eng_text_2[0:5]\n",
    "print(sample_tel_2)\n",
    "print()\n",
    "print(sample_eng_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<start> speaking about her all time favourite cricketer , deepika said its former india skipper rahul dravid . <end>',\n",
       " '<start> debris has to be removed . <end>',\n",
       " '<start> sridevi s sudden demise sent shock waves throughout the country . <end>',\n",
       " '<start> this is one such attempt . <end>',\n",
       " '<start> megapixel front camera <end>')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c)!='BN')\n",
    "\n",
    "# Remove punctuations, reduce words to lower case,and adding '<start>' and '<end>' tokens to the sentences\n",
    "\n",
    "def preprocess_eng(w):\n",
    "    #w= unicode_to_ascii(w)\n",
    "    w= w.lower().strip()\n",
    "    w= re.sub(r'([?.!,¿_])',r' \\1 ',w)\n",
    "    w= re.sub(r'[\" \"]+', \" \",w) # keep white space\n",
    "    w= re.sub(r\"[^a-zA-Z?.!,¿_]+\",\" \",w)\n",
    "    w= w.strip()\n",
    "    w= '<start> '+ w +' <end>'\n",
    "    return w\n",
    "\n",
    "def preprocess_tel(w):\n",
    "    w= unicode_to_ascii(w)\n",
    "    w= re.sub(r'([\\?.!,¿_])',r' \\1 ',w)\n",
    "    w= re.sub(r'[\" \"]+', \" \",w) # keep white space\n",
    "    w= w.strip()\n",
    "    w= '<start> '+ w +' <end>'\n",
    "    w= w.replace(\"u'\", \"'\")\n",
    "    return w\n",
    "\n",
    "line=[preprocess_eng(i) for i in sample_eng_1]\n",
    "tuple(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eng_dataset(filepath):\n",
    "    #limit num_examples for faster training - len(lines) is full data\n",
    "    lines= io.open(filepath,encoding='UTF-8').read().strip().split('\\n')\n",
    "    lines=[preprocess_eng(i) for i in lines]\n",
    "    return tuple(lines)\n",
    "\n",
    "\n",
    "def create_tel_dataset(filepath):\n",
    "    #limit num_examples for faster training - len(lines) is full data\n",
    "    if filepath==filepath_te_1:\n",
    "        lines= io.open(filepath,encoding='UTF-8').read().strip().split('\\n')\n",
    "    else:\n",
    "        lines= io.open(filepath,encoding='UTF-16').read().strip().split('\\n')\n",
    "    lines=[preprocess_tel(i) for i in lines]\n",
    "    return tuple(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize function \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = Tokenizer(filters='',oov_token='<OOV>')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    lang_data = lang_tokenizer.texts_to_sequences(lang)\n",
    "    lang_data = pad_sequences(lang_data,padding=\"post\")\n",
    "    return lang_data, lang_tokenizer\n",
    "\n",
    "def load_dataset(filepath1,filepath2):\n",
    "    # create cleaned input and target sentences\n",
    "    input_lang = create_eng_dataset(filepath1)\n",
    "    targ_lang  = create_tel_dataset(filepath2)\n",
    "    input_lang, inp_lang_tokenizer  = tokenize(input_lang)\n",
    "    target_lang, targ_lang_tokenizer= tokenize(targ_lang)\n",
    "    return input_lang,target_lang,inp_lang_tokenizer,targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_lang_1,target_lang_1,inp_lang_tokenizer_1,targ_lang_tokenizer_1 = load_dataset(filepath_en_1,filepath_te_1)\n",
    "\n",
    "input_lang_2,target_lang_2,inp_lang_tokenizer_2,targ_lang_tokenizer_2 = load_dataset(filepath_en_2,filepath_te_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92,)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang_1.shape,target_lang_1.shape\n",
    "input_lang_2.shape,target_lang_2.shape\n",
    "input_lang_1[0].shape\n",
    "input_lang_2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_train_1, input_val_1, target_train_1, target_val_1 = train_test_split(input_lang_1, target_lang_1, test_size=0.2)\n",
    "input_train_2, input_val_2, target_train_2, target_val_2 = train_test_split(input_lang_2, target_lang_2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3416, 92), (3416, 75))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train_1.shape,target_train_1.shape\n",
    "input_train_2.shape,target_train_2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(64, 92), dtype=tf.int32, name=None), TensorSpec(shape=(64, 75), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer_size = 5000\n",
    "batch_size  = 64\n",
    "\n",
    "train_dataset_1 = tf.data.Dataset.from_tensor_slices((input_train_1,target_train_1))\n",
    "train_dataset_1 = train_dataset_1.shuffle(buffer_size).batch(batch_size,drop_remainder =True)\n",
    "\n",
    "train_dataset_2 = tf.data.Dataset.from_tensor_slices((input_train_2,target_train_2))\n",
    "train_dataset_2 = train_dataset_2.shuffle(buffer_size).batch(batch_size,drop_remainder =True)\n",
    "\n",
    "train_dataset_1\n",
    "train_dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_1 = tf.data.Dataset.from_tensor_slices((input_val_1,target_val_1))\n",
    "val_dataset_1 = val_dataset_1.batch(batch_size,drop_remainder =True)\n",
    "\n",
    "val_dataset_2 = tf.data.Dataset.from_tensor_slices((input_val_2,target_val_2))\n",
    "val_dataset_2 = val_dataset_2.batch(batch_size,drop_remainder =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset1: max_length_english, max_length_telugu, vocab_size_english, vocab_size_telugu\n",
      "88 95 7962 15670\n",
      "Dataset2: max_length_english, max_length_telugu, vocab_size_english, vocab_size_telugu\n",
      "92 75 5947 11786\n"
     ]
    }
   ],
   "source": [
    "ex_input_batch_1,ex_tar_batch_1 = next(iter(train_dataset_1))\n",
    "ex_input_batch_1.shape,ex_tar_batch_1.shape\n",
    "\n",
    "maxlen_input_1 = ex_input_batch_1.shape[1]\n",
    "maxlen_output_1   = ex_tar_batch_1.shape[1]\n",
    "\n",
    "vocab_inp_size_1 = len(inp_lang_tokenizer_1.word_index)+1\n",
    "vocab_tar_size_1 = len(targ_lang_tokenizer_1.word_index)+1\n",
    "\n",
    "ex_input_batch_2,ex_tar_batch_2 = next(iter(train_dataset_2))\n",
    "ex_input_batch_2.shape,ex_tar_batch_2.shape\n",
    "\n",
    "maxlen_input_2 = ex_input_batch_2.shape[1]\n",
    "maxlen_output_2   = ex_tar_batch_2.shape[1]\n",
    "\n",
    "vocab_inp_size_2 = len(inp_lang_tokenizer_2.word_index)+1\n",
    "vocab_tar_size_2 = len(targ_lang_tokenizer_2.word_index)+1\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "steps_per_epoch_1 = 5000//batch_size\n",
    "steps_per_epoch_2= 4270//batch_size\n",
    "\n",
    "print(\"Dataset1: max_length_english, max_length_telugu, vocab_size_english, vocab_size_telugu\")\n",
    "print(maxlen_input_1, maxlen_output_1, vocab_inp_size_1, vocab_tar_size_1)\n",
    "\n",
    "print(\"Dataset2: max_length_english, max_length_telugu, vocab_size_english, vocab_size_telugu\")\n",
    "print(maxlen_input_2, maxlen_output_2, vocab_inp_size_2, vocab_tar_size_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, h, c = self.lstm_layer(x, initial_state=hidden)\n",
    "        return output, h, c\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return [tf.zeros((self.batch_size, self.enc_units)), tf.zeros((self.batch_size, self.enc_units))]  # all zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1\n",
      "Encoder output shape: (batch size, sequence length, units) (64, 88, 512)\n",
      "Encoder h vecotr shape: (batch size, units) (64, 512)\n",
      "Encoder c vector shape: (batch size, units) (64, 512)\n",
      "Dataset 2\n",
      "Encoder output shape: (batch size, sequence length, units) (64, 92, 512)\n",
      "Encoder h vecotr shape: (batch size, units) (64, 512)\n",
      "Encoder c vector shape: (batch size, units) (64, 512)\n"
     ]
    }
   ],
   "source": [
    "## Test Encoder Stack\n",
    "\n",
    "encoder_1 = Encoder(vocab_inp_size_1, embedding_dim, units,batch_size)\n",
    "encoder_2 = Encoder(vocab_inp_size_2, embedding_dim, units,batch_size)\n",
    "\n",
    "# sample input\n",
    "sample_hidden_1 = encoder_1.initialize_hidden_state()\n",
    "sample_output_1, sample_h_1, sample_c_1 = encoder_1(ex_input_batch_1, sample_hidden_1)\n",
    "print ('Dataset 1')\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output_1.shape))\n",
    "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h_1.shape))\n",
    "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c_1.shape))\n",
    "\n",
    "\n",
    "\n",
    "# sample input\n",
    "sample_hidden_2 = encoder_2.initialize_hidden_state()\n",
    "sample_output_2, sample_h_2, sample_c_2 = encoder_2(ex_input_batch_2, sample_hidden_2)\n",
    "print ('Dataset 2')\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output_2.shape))\n",
    "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h_2.shape))\n",
    "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c_2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000001CC8AD90AD0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1168, in map_structure\n",
      "    return _tf_core_map_structure(func, *structure, **kwargs)  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1206, in _tf_core_map_structure\n",
      "    return _tf_core_pack_sequence_as(  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1208, in <listcomp>\n",
      "    [func(*x) for x in entries],  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 506, in <lambda>\n",
      "    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000001CC8AD91610>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1168, in map_structure\n",
      "    return _tf_core_map_structure(func, *structure, **kwargs)  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1206, in _tf_core_map_structure\n",
      "    return _tf_core_pack_sequence_as(  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1208, in <listcomp>\n",
      "    [func(*x) for x in entries],  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 506, in <lambda>\n",
      "    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000001CC8ADEC6D0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1168, in map_structure\n",
      "    return _tf_core_map_structure(func, *structure, **kwargs)  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1206, in _tf_core_map_structure\n",
      "    return _tf_core_pack_sequence_as(  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1208, in <listcomp>\n",
      "    [func(*x) for x in entries],  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 506, in <lambda>\n",
      "    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000001CC8ACE4E50>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1168, in map_structure\n",
      "    return _tf_core_map_structure(func, *structure, **kwargs)  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1206, in _tf_core_map_structure\n",
      "    return _tf_core_pack_sequence_as(  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1208, in <listcomp>\n",
      "    [func(*x) for x in entries],  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_addons\\seq2seq\\decoder.py\", line 506, in <lambda>\n",
      "    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4884"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "units=512\n",
    "batch_size =64\n",
    "maxlen_output_1 = target_train_1.shape[1]\n",
    "maxlen_output_2 = target_train_2.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.units = units\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.decoder_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #Final Dense layer on which softmax will be applied\n",
    "        self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # Define the fundamental cell for decoder recurrent structure\n",
    "        self.decoder_inner_cell = tf.keras.layers.LSTMCell(self.units)\n",
    "        \n",
    "        # Sampler\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "        \n",
    "        ## Attention Wrapper ##\n",
    "        self.attention_mechanism = tfa.seq2seq.attention_wrapper.LuongAttention(units,memory=None,\n",
    "                                                                                memory_sequence_length=self.batch_size*[maxlen_input_2])\n",
    "        self.attention_decoder_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper(cell=self.decoder_inner_cell, \n",
    "                                                                                     attention_mechanism=self.attention_mechanism,\n",
    "                                                                                     attention_layer_size=self.units)\n",
    "        \n",
    "        ## Basic Decoder ##\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(cell=self.attention_decoder_cell,sampler = self.sampler,\n",
    "                                               output_layer = self.output_layer)\n",
    "        \n",
    "         \n",
    "        self.inference_decoder = tfa.seq2seq.BasicDecoder(cell=self.attention_decoder_cell,\n",
    "                                                          sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
    "                                                              embedding_fn = self.decoder_embedding),\n",
    "                                                          output_layer=self.output_layer,maximum_iterations = 154)\n",
    "        \n",
    "    def build_initial_state(self, batch_size, encoder_state, Dtype):\n",
    "        decoder_initial_state = self.attention_decoder_cell.get_initial_state(batch_size=batch_size, dtype=Dtype)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "        return decoder_initial_state\n",
    "        \n",
    "    def call(self,decoder_input,initial_state):\n",
    "        decoder_embeddings = self.decoder_embedding(decoder_input)\n",
    "        outputs, _, _ = self.decoder(decoder_embeddings,initial_state=initial_state,\n",
    "                                     sequence_length=self.batch_size*[maxlen_output_2-1])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder1 Outputs Shape:  (64, 94, 15670)\n"
     ]
    }
   ],
   "source": [
    "# Test decoder stack\n",
    "\n",
    "decoder_1 = Decoder(vocab_tar_size_1, embedding_dim,512, batch_size)\n",
    "sample_x_1 = tf.random.uniform((batch_size, maxlen_output_1))\n",
    "decoder_1.attention_mechanism.setup_memory(sample_output_1) # from encoder\n",
    "initial_state_1 = decoder_1.build_initial_state(batch_size, [sample_h_1, sample_c_1], tf.float32)\n",
    "\n",
    "sample_decoder_outputs_1 = decoder_1(sample_x_1, initial_state_1)\n",
    "\n",
    "print(\"Decoder1 Outputs Shape: \", sample_decoder_outputs_1.rnn_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # real shape = (batch_size, maxlen_output)\n",
    "    # pred shape = (batch_size, maxlen_output, vocab_tar_size )\n",
    "    cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "    mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)  \n",
    "    loss = mask* loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_1 = './training_checkpoints_1'\n",
    "checkpoint_prefix_1 = os.path.join(checkpoint_dir_1, \"ckpt\")\n",
    "checkpoint_1 = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder_1,\n",
    "                                 decoder=decoder_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(encoder, decoder,inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "        \n",
    "        dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
    "        real = targ[ : , 1: ]         # ignore <start> token\n",
    "        \n",
    "        # Set the AttentionMechanism object with encoder_outputs\n",
    "        decoder.attention_mechanism.setup_memory(enc_output)\n",
    "        # Create AttentionWrapperState as initial_state for decoder\n",
    "        decoder_initial_state = decoder.build_initial_state(batch_size, [enc_h, enc_c], tf.float32)\n",
    "        pred = decoder(dec_input, decoder_initial_state)\n",
    "        logits = pred.rnn_output\n",
    "        loss = loss_function(real, logits)\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37136"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.9425\n",
      "Epoch 1 Loss 0.6348\n",
      "Time taken for 1 epoch 923.1122658252716 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.7289\n",
      "Epoch 2 Loss 0.5566\n",
      "Time taken for 1 epoch 998.2482016086578 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.6669\n",
      "Epoch 3 Loss 0.5302\n",
      "Time taken for 1 epoch 1058.987454175949 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.6493\n",
      "Epoch 4 Loss 0.5123\n",
      "Time taken for 1 epoch 1104.291584968567 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.5950\n",
      "Epoch 5 Loss 0.4930\n",
      "Time taken for 1 epoch 1125.251837015152 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.5934\n",
      "Epoch 6 Loss 0.4735\n",
      "Time taken for 1 epoch 1153.67276096344 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.5464\n",
      "Epoch 7 Loss 0.4525\n",
      "Time taken for 1 epoch 1191.7470231056213 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.5687\n",
      "Epoch 8 Loss 0.4257\n",
      "Time taken for 1 epoch 1236.8411598205566 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.4662\n",
      "Epoch 9 Loss 0.3987\n",
      "Time taken for 1 epoch 1293.501158952713 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.4343\n",
      "Epoch 10 Loss 0.3687\n",
      "Time taken for 1 epoch 1332.2530987262726 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.4309\n",
      "Epoch 11 Loss 0.3359\n",
      "Time taken for 1 epoch 1333.5900781154633 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.2875\n",
      "Epoch 12 Loss 0.3012\n",
      "Time taken for 1 epoch 1355.6618711948395 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.2922\n",
      "Epoch 13 Loss 0.2687\n",
      "Time taken for 1 epoch 1368.6864972114563 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.2505\n",
      "Epoch 14 Loss 0.2331\n",
      "Time taken for 1 epoch 1399.2182114124298 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.2237\n",
      "Epoch 15 Loss 0.2004\n",
      "Time taken for 1 epoch 1408.2077867984772 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.1631\n",
      "Epoch 16 Loss 0.1717\n",
      "Time taken for 1 epoch 1397.466969013214 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.1923\n",
      "Epoch 17 Loss 0.1459\n",
      "Time taken for 1 epoch 1411.7835977077484 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.1364\n",
      "Epoch 18 Loss 0.1235\n",
      "Time taken for 1 epoch 1430.3075399398804 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.1154\n",
      "Epoch 19 Loss 0.1053\n",
      "Time taken for 1 epoch 1428.9774129390717 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0803\n",
      "Epoch 20 Loss 0.0912\n",
      "Time taken for 1 epoch 1421.5855178833008 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    enc_hidden = encoder_1.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset_1.take(steps_per_epoch_1)):\n",
    "        batch_loss = train_step(encoder_1,decoder_1,inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,batch,batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint_1.save(file_prefix = checkpoint_prefix_1)\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,total_loss / steps_per_epoch_1))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_evaluate_sentence(sentence,encoder,decoder,inp_lang_tokenizer,maxlen_input,targ_lang_tokenizer,beam_width=3):\n",
    "    y = str(sentence)\n",
    "    y = preprocess_eng(y)\n",
    "    sentence= str(y)\n",
    "\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.strip().split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=maxlen_input,padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "    \n",
    "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "    \n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "    \n",
    "    start_tokens = tf.fill([inference_batch_size], targ_lang_tokenizer.word_index['<start>'])\n",
    "    end_token = targ_lang_tokenizer.word_index['<end>']\n",
    "    \n",
    "    enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n",
    "    decoder.attention_mechanism.setup_memory(enc_out)\n",
    "    print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 194, 512]] :\", enc_out.shape)\n",
    "    \n",
    "    # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
    "    hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n",
    "    decoder_initial_state = decoder.attention_decoder_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n",
    "    decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n",
    "    \n",
    "    # Instantiate BeamSearchDecoder\n",
    "    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.attention_decoder_cell,beam_width=beam_width, output_layer=decoder.output_layer)\n",
    "    decoder_embedding_matrix = decoder.decoder_embedding.variables[0]\n",
    "    \n",
    "    # The BeamSearchDecoder object's call() function takes care of everything.\n",
    "    outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, \n",
    "                                                              start_tokens=start_tokens, end_token=end_token, \n",
    "                                                              initial_state=decoder_initial_state)\n",
    "    \n",
    "    final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n",
    "    beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n",
    "    return final_outputs.numpy(), beam_scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_translate(sentence, targ_lang_tokenizer):\n",
    "    result, beam_scores = beam_evaluate_sentence(sentence,encoder_1,decoder_1,inp_lang_tokenizer_1,maxlen_input_1,targ_lang_tokenizer)\n",
    "    print(result.shape, beam_scores.shape)\n",
    "    print()\n",
    "    \n",
    "    for beam, score in zip(result, beam_scores):\n",
    "        print(beam.shape, score.shape)\n",
    "        print()\n",
    "        \n",
    "    output = targ_lang_tokenizer.sequences_to_texts(beam)\n",
    "    output = [a[:a.index('<end>')] for a in output]\n",
    "    beam_score = [a.sum() for a in score]\n",
    "    print('Input: %s' % (sentence))\n",
    "    print()\n",
    "    \n",
    "    for i in range(len(output)):\n",
    "        print('{} Predicted translation: {}  {}'.format(i+1, output[i], beam_score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1:\n",
      "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 194, 512]] : (3, 88, 512)\n",
      "(1, 3, 4) (1, 3, 4)\n",
      "\n",
      "(3, 4) (3, 4)\n",
      "\n",
      "Input: i could not stand it.\n",
      "\n",
      "1 Predicted translation: అమ్మగా తట్టుకోలేకపోయా .   -1.2535946369171143\n",
      "2 Predicted translation: ఎంత తట్టుకోలేకపోయా .   -14.54857349395752\n",
      "3 Predicted translation: సమాధానమూ తెలియదు .   -15.547178268432617\n",
      "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 194, 512]] : (3, 88, 512)\n",
      "(1, 3, 6) (1, 3, 6)\n",
      "\n",
      "(3, 6) (3, 6)\n",
      "\n",
      "Input: the officials were shocked.\n",
      "\n",
      "1 Predicted translation: దీంతో అధికారులు అలెర్ట్ అయ్యారు .   -4.277356147766113\n",
      "2 Predicted translation: దీంతో అధికారులు దిగొచ్చారు .   -18.412607192993164\n",
      "3 Predicted translation: దీంతో అధికారులు అలెర్ట్ మరమ్మత్తు .   -21.522920608520508\n"
     ]
    }
   ],
   "source": [
    "print('dataset1:')\n",
    "beam_translate(u'i could not stand it.',targ_lang_tokenizer_1)\n",
    "\n",
    "beam_translate(u'the officials were shocked.',targ_lang_tokenizer_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data (if necessary)\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def evaluate_sen(sentence,encoder,decoder,inp_lang_tokenizer,maxlen_input,targ_lang_tokenizer):\n",
    "    y = str(sentence)\n",
    "    y = preprocess_eng(y)\n",
    "    sentence= str(y)\n",
    "\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=maxlen_input,padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "    \n",
    "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "    \n",
    "    start_tokens = tf.fill([inference_batch_size], targ_lang_tokenizer.word_index['<start>'])\n",
    "    end_token = targ_lang_tokenizer.word_index['<end>']\n",
    "    \n",
    "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "    \n",
    "    # Instantiate BasicDecoder object\n",
    "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.attention_decoder_cell, sampler=greedy_sampler, \n",
    "                                                output_layer=decoder.output_layer)\n",
    "    \n",
    "    # Setup Memory in decoder stack\n",
    "    decoder.attention_mechanism.setup_memory(enc_out)\n",
    "    \n",
    "    # set decoder_initial_state\n",
    "    decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
    "    decoder_embedding_matrix = decoder.decoder_embedding.variables[0]\n",
    "    \n",
    "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, \n",
    "                                     end_token= end_token, initial_state=decoder_initial_state)\n",
    "    return outputs.sample_id.numpy()\n",
    "\n",
    "\n",
    "def translate2(sentence,targ_lang_tokenizer,reference):\n",
    "  result = evaluate_sen(sentence,encoder_1,decoder_1,inp_lang_tokenizer_1,maxlen_input_1,targ_lang_tokenizer)\n",
    "  print(result)\n",
    "  result = targ_lang_tokenizer.sequences_to_texts(result)\n",
    "  \n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "  \n",
    "  # Calculate BLEU score\n",
    "  bleu = calculate_bleu_score(reference, result)\n",
    "  print(f\"BLEU Score: {bleu:.4f}\")\n",
    "  \n",
    "\n",
    "def calculate_bleu_score(reference, prediction):\n",
    "    \n",
    "    if isinstance(reference, list):\n",
    "        reference = \" \".join(reference)\n",
    "    if isinstance(prediction, list):\n",
    "        prediction = \" \".join(prediction)\n",
    "    prediction = prediction.replace('<end>', '').strip()\n",
    "    # Tokenize the reference and prediction sentences\n",
    "    reference_tokens = word_tokenize(reference)\n",
    "    prediction_tokens = word_tokenize(prediction)\n",
    "\n",
    "    # Wrap reference tokens in a list (BLEU requires list of lists for references)\n",
    "    references = [reference_tokens]\n",
    "\n",
    "    # Calculate the BLEU score using sentence_bleu\n",
    "    bleu_score = sentence_bleu(references, prediction_tokens)\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1:\n",
      "[[  31 2343 2344 4427    4    3]]\n",
      "Input: he is a student of iit at bombay.\n",
      "Predicted translation: ['దీంతో ముంబయిలో ఐఐటి చదువుతున్నాడు . <end>']\n",
      "BLEU Score: 0.6687\n",
      "[[2249 4128 2249 1547 4129    4    3]]\n",
      "Input: demand equal pay for equal work.\n",
      "Predicted translation: ['సమాన పనికి సమాన వేతనం ఇవ్వాలన్నారు . <end>']\n",
      "BLEU Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print('dataset1:')\n",
    "# Example references (true translations, already in Telugu)\n",
    "reference1 = 'బాబు ముంబయిలో ఐఐటి చదువుతున్నాడు.'\n",
    "reference2='సమాన పనికి సమాన వేతనం ఇవ్వాలన్నారు.'\n",
    "translate2(u'he is a student of iit at bombay.',targ_lang_tokenizer_1,reference1)\n",
    "translate2(u'demand equal pay for equal work.',targ_lang_tokenizer_1,reference2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
